from chiscore import optimal_davies_pvalue


def _mod_liu(q, w):
    from chiscore import liu_sf

    (pv, dof_x, _, info) = liu_sf(q, w, [1] * len(w), [0] * len(w), True)
    return (pv, info["mu_q"], info["sigma_q"], dof_x)


class StructLMM:
    r"""
    Structured linear mixed model that accounts for genotype-environment interactions.

    StructLMM [MC18]_ extends the conventional linear mixed model by including an
    additional per-individual effect term that accounts for genotype-environment
    interaction, which can be represented as an n√ó1 vector, ùõÉ.
    The model is given by

        ùê≤ = ùôºùõÇ + ùê†ùõΩ + ùê†‚äôùõÉ + ùêû + ùõÜ,

    where

        ùõÉ ‚àº ùìù(ùüé, ùìã‚ÇÄ(1-œÅ)ùô¥ùô¥·µÄ), ùêû ‚àº ùìù(ùüé, ùìã‚ÇÅùöÜùöÜ·µÄ), and ùõÜ ‚àº ùìù(ùüé, ùìã‚ÇÇùô∏).

    The arrays ùê≤, ùôº, ùê†, ùô¥, and ùöÜ are given by the user.

    The ùõΩ term is considered a fixed or random effect depending on the test being
    employed.
    For the iteraction test, ùõΩ is considered a fixed-effect term, while

        ùõΩ ‚àº ùìù(0, ùìã‚ÇÄ‚ãÖœÅ)

    for the association test.
    Since the model for interaction test can be defined from the model for associaton
    test by setting ùôº‚Üê[ùôº ùê†] and œÅ=0, we will show the mathematical derivations for the
    latter.
    Therefore, consider the general model

        ùê≤ = ùôºùõÇ + ùê†‚äôùõÉ + ùêû + ùõÜ,

    where ùõÉ ‚àº ùìù(ùüé, ùìã‚ÇÄ(œÅùüèùüè·µÄ + (1-œÅ)ùô¥ùô¥·µÄ)).
    Equivalently, we have

        ùê≤ ‚àº ùìù(ùôºùõÇ, ùìã‚ÇÄùô≥(œÅùüèùüè·µÄ + (1-œÅ)ùô¥ùô¥·µÄ)ùô≥ + ùìã‚ÇÅùöÜùöÜ·µÄ + ùìã‚ÇÇùô∏),

    where ùô≥ = diag(ùê†).

    The null hypothesis emerges when we set ùìã‚ÇÄ=0.
    Let

        ùô∫‚ÇÄ = ùìã‚ÇÅùöÜùöÜ·µÄ + ùìã‚ÇÇùô∏

    for optimal ùìã‚ÇÅ, ùìã‚ÇÇ, and ùõÇ under the null hypothesis.
    The score-based test statistic is given by

        ùëÑ = ¬Ωùê≤·µÄùôø‚ÇÄ(‚àÇùô∫)ùôø‚ÇÄùê≤,

    where

        ùôø‚ÇÄ = ùô∫‚ÇÄ‚Åª¬π - ùô∫‚ÇÄ‚Åª¬πùôº(ùôº·µÄùô∫‚ÇÄ‚Åª¬πùôº)‚Åª¬πùôº·µÄùô∫‚ÇÄ‚Åª¬π.


    000000000000000000000000000000000000000000000000000000000000000000000000000000000000
    while the association model is given by

        ùê≤ = ùôºùõÇ + ùê†‚äôùõÉ + ùêû + ùõÜ,

    where

        ùõÉ‚àºùìù(ùüé, ùìã‚ÇÄ(œÅùüèùüè·µÄ + (1-œÅ)ùô¥ùô¥·µÄ)), ùêû‚àºùìù(ùüé, ùìã‚ÇÅùöÜùöÜ·µÄ), and ùõÜ‚àºùìù(ùüé, ùìã‚ÇÇùô∏).

    The parameters of the model are œÅ, ùìã‚ÇÄ, ùìã‚ÇÅ, and ùìã‚ÇÇ.

    The null model is given by

        ùê≤ ‚àº ùìù(ùôºùõÇ, ùìã‚ÇÅùöÜùöÜ·µÄ + ùìã‚ÇÇùô∏).

    Let ùô∫‚ÇÄ = ùìã‚ÇÅùöÜùöÜ·µÄ + ùìã‚ÇÇùô∏ be the covariance matrix for the null model.

    Let P

    The above equation can be simplified by noticing that ùô≥ùüèùüè·µÄùô≥ = ùê†ùê†·µÄ.

    References
    ----------
    .. [MC18] Moore, R., Casale, F. P., Bonder, M. J., Horta, D., Franke, L., Barroso,
       I., & Stegle, O. (2018). A linear mixed-model approach to study multivariate
       gene‚Äìenvironment interactions (p. 1). Nature Publishing Group.
    .. [LI14] Lippert, C., Xiang, J., Horta, D., Widmer, C., Kadie, C., Heckerman, D.,
       & Listgarten, J. (2014). Greater power and computational efficiency for
       kernel-based association testing of sets of genetic variants. Bioinformatics,
       30(22), 3206-3214.
    """

    def __init__(self, y, M, E, W=None):
        from numpy import sqrt, asarray, atleast_2d
        from numpy_sugar import ddot

        self._y = atleast_2d(asarray(y, float).ravel()).T
        self._E = atleast_2d(asarray(E, float).T).T

        if W is None:
            self._W = self._E
        elif isinstance(W, tuple):
            # W must be an eigen-decomposition of ùöÜùöÜ·µÄ
            self._W = ddot(W[0], sqrt(W[1]))
        else:
            self._W = atleast_2d(asarray(W, float).T).T

        self._M = atleast_2d(asarray(M, float).T).T

        nsamples = len(self._y)
        if nsamples != self._M.shape[0]:
            raise ValueError("Number of samples mismatch between y and M.")

        if nsamples != self._E.shape[0]:
            raise ValueError("Number of samples mismatch between y and E.")

        if nsamples != self._W.shape[0]:
            raise ValueError("Number of samples mismatch between y and W.")

        self._lmm = None

        self._rhos = [0.0, 0.1 ** 2, 0.2 ** 2, 0.3 ** 2, 0.4 ** 2, 0.5 ** 2, 0.5, 1.0]

    def fit(self, verbose=True):
        from glimix_core.lmm import Kron2Sum

        self._lmm = Kron2Sum(self._y, [[1]], self._M, self._W, restricted=True)
        self._lmm.fit(verbose=verbose)
        self._covarparam0 = self._lmm.C0[0, 0]
        self._covarparam1 = self._lmm.C1[0, 0]

    def _xBy(self, rho, y, x):
        """
        Let ùô± = œÅùüè + (1-œÅ)ùô¥ùô¥·µÄ.
        It computes ùê≤·µÄùô±ùê±.
        """
        l = rho * (y.sum() * x.sum())
        r = (1 - rho) * (y.T @ self._E) @ (self._E.T @ x)
        return l + r

    def _P(self, M):
        """
        Let ùô∫‚ÇÄ be the optimal covariance matrix under the null hypothesis.
        Given ùôº, this method computes

            ùôø‚ÇÄ = ùô∫‚ÇÄ‚Åª¬π - ùô∫‚ÇÄ‚Åª¬πùôº(ùôº·µÄùô∫‚ÇÄ‚Åª¬πùôº)‚Åª¬πùôº·µÄùô∫‚ÇÄ‚Åª¬π.
        """
        from numpy_sugar.linalg import rsolve
        from scipy.linalg import cho_solve

        RV = rsolve(self._lmm.covariance(), M)
        if self._lmm.X is not None:
            WKiM = self._lmm.M.T @ RV
            terms = self._lmm._terms
            WAiWKiM = self._lmm.X @ cho_solve(terms["Lh"], WKiM)
            KiWAiWKiM = rsolve(self._lmm.covariance(), WAiWKiM)
            RV -= KiWAiWKiM

        return RV

    def _score_stats(self, g):
        """
        Let ùô∫‚ÇÄ be the optimal covariance matrix under the null hypothesis.
        The score-based test statistic is given by

            ùëÑ = ¬Ωùê≤·µÄùôø‚ÇÄ(‚àÇùô∫)ùôø‚ÇÄùê≤,

        where

            ‚àÇùô∫ = ùô≥(œÅùüèùüè·µÄ + (1-œÅ)ùô¥ùô¥·µÄ)ùô≥

        and ùô≥ = diag(ùê†).
        """
        from numpy_sugar import ddot
        from numpy import zeros

        Q = zeros(len(self._rhos))
        DPy = ddot(g, self._P(self._y))
        for i in range(len(self._rhos)):
            rho = self._rhos[i]
            Q[i] = self._xBy(rho, DPy, DPy) / 2

        return Q

    def _score_stats_null_dist(self, g):
        """
        Under the null hypothesis, the score-based test statistic follows a weighted sum
        of random variables:

            ùëÑ ‚àº ‚àë·µ¢ùúÜ·µ¢œá¬≤(1),

        where ùúÜ·µ¢ are the non-zero eigenvalues of ¬Ω‚àöùôø‚ÇÄ(‚àÇùô∫)‚àöùôø‚ÇÄ.

        Note that

            ‚àÇùô∫ = ùô≥(œÅùüèùüè·µÄ + (1-œÅ)ùô¥ùô¥·µÄ)ùô≥ = (œÅùê†ùê†·µÄ + (1-œÅ)ùô¥ÃÉùô¥ÃÉ·µÄ)

        for ùô¥ÃÉ = ùô≥ùô¥.
        By using SVD decomposition, one can show that the non-zero eigenvalues of ùöáùöá·µÄ
        are equal to the non-zero eigenvalues of ùöá·µÄùöá.
        Therefore, ùúÜ·µ¢ are the non-zero eigenvalues of

            ¬Ω[‚àöœÅùê† ‚àö(1-œÅ)ùô¥ÃÉ]ùôø‚ÇÄ[‚àöœÅùê† ‚àö(1-œÅ)ùô¥ÃÉ]·µÄ.

        """
        from numpy import empty
        from numpy.linalg import eigvalsh
        from math import sqrt
        from numpy_sugar import ddot

        Et = ddot(g, self._E)
        Pg = self._P(g)
        PEt = self._P(Et)

        gPg = g.T @ Pg
        EtPEt = Et.T @ PEt
        gPEt = g.T @ PEt

        n = Et.shape[1] + 1
        F = empty((n, n))

        lambdas = []
        for i in range(len(self._rhos)):
            rho = self._rhos[i]

            F[0, 0] = rho * gPg
            F[0, 1:] = sqrt(rho) * sqrt(1 - rho) * gPEt
            F[1:, 0] = F[0, 1:]
            F[1:, 1:] = (1 - rho) * EtPEt

            lambdas.append(eigvalsh(F) / 2)

        return lambdas

    def _score_stats_pvalue(self, Qs, lambdas):
        """
        Computes Pr(ùëÑ > q) for ùëÑ ‚àº ‚àë·µ¢ùúÜ·µ¢œá¬≤(1).

        Pr(ùëÑ > q) is the p-value for the score statistic.

        Parameters
        ----------
        Qs : array_like
            ùëÑ from the null distribution.
        lambdas : array_like
            ùúÜ·µ¢ from the null distribution.
        """
        from numpy import stack

        pvals = []
        for Q, lam in zip(Qs, lambdas):
            pvals.append(_mod_liu(Q, lam))

        return stack(pvals, axis=0)

    def _qmin(self, pliumod):
        from numpy import zeros
        import scipy.stats as st

        # T statistic
        T = pliumod[:, 0].min()

        # 2. Calculate qmin
        qmin = zeros(len(self._rhos))
        percentile = 1 - T
        for i in range(len(self._rhos)):
            q = st.chi2.ppf(percentile, pliumod[i, 3])
            # Recalculate p-value for each Q rho of seeing values at least as
            # extreme as q again using the modified matching moments method
            qmin[i] = (q - pliumod[i, 3]) / (2 * pliumod[i, 3]) ** 0.5 * pliumod[
                i, 2
            ] + pliumod[i, 1]
            pass

        return qmin

    def score_2_dof(self, X):
        from numpy import trace, sum
        import scipy as sp
        import scipy.linalg as la

        Q_rho = self._score_stats(X.ravel())

        if len(self._rhos) == 1:
            raise NotImplementedError("We have not tested it yet.")

        null_lambdas = self._score_stats_null_dist(X.ravel())
        pliumod = self._score_stats_pvalue(Q_rho, null_lambdas)
        qmin = self._qmin(pliumod)

        # 3. Calculate quantites that occur in null distribution
        Px1 = self._P(X)
        m = 0.5 * (X.T @ Px1)
        xoE = X * self._E
        PxoE = self._P(xoE)
        ETxPxE = 0.5 * (xoE.T @ PxoE)
        ETxPx1 = xoE.T @ Px1
        ETxPx11xPxE = 0.25 / m * (ETxPx1 @ ETxPx1.T)
        ZTIminusMZ = ETxPxE - ETxPx11xPxE
        eigh, _ = la.eigh(ZTIminusMZ)

        eta = ETxPx11xPxE @ ZTIminusMZ
        vareta = 4 * trace(eta)

        OneZTZE = 0.5 * (X.T @ PxoE)
        tau_top = OneZTZE @ OneZTZE.T
        tau_rho = sp.zeros(len(self._rhos))
        for i in range(len(self._rhos)):
            tau_rho[i] = self._rhos[i] * m + (1 - self._rhos[i]) / m * tau_top

        MuQ = sum(eigh)
        VarQ = sum(eigh ** 2) * 2 + vareta
        KerQ = sum(eigh ** 4) / (sum(eigh ** 2) ** 2) * 12
        Df = 12 / KerQ

        # 4. Integration
        T = pliumod[:, 0].min()
        pvalue = optimal_davies_pvalue(
            qmin, MuQ, VarQ, KerQ, eigh, vareta, Df, tau_rho, self._rhos, T
        )

        # Final correction to make sure that the p-value returned is sensible
        multi = 3
        if len(self._rhos) < 3:
            multi = 2
        idx = sp.where(pliumod[:, 0] > 0)[0]
        pval = pliumod[:, 0].min() * multi
        if pvalue <= 0 or len(idx) < len(self._rhos):
            pvalue = pval
        if pvalue == 0:
            if len(idx) > 0:
                pvalue = pliumod[:, 0][idx].min()

        return pvalue
